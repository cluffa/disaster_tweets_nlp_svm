---
title: "NLP"
output: github_document
---

This is project to introduce myself to NLP in R.

With Kaggle disaster tweets dataset:

[Natural Language Processing with Disaster Tweets \| Kaggle](https://www.kaggle.com/c/nlp-getting-started)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, cache.comments = FALSE)
library(tidyverse)
library(tm)
set.seed(1)
```

```{r}
tweets <- read_csv('./train.csv', show_col_types = FALSE)
glimpse(tweets)
mean(tweets$target)
```

This is close enough to 0.5 that I will still use an accuracy score. If it becomes a problem I might use f1 score.

I will use object type corpus from the tm package to process the data. A corpus is a collection of text style character strings or documents.

```{r}
Corpus(VectorSource(tweets$text))
```

```{r warning=FALSE}
# using tm package

preprocess_tweets <- function(text, prob = 0.999, extra_blacklist = c()) {
  # Sort of One-Hot-Encoding but for entire tweet and encodes each word,
  # removes sparse words
  # input char vector of tweets, prob, extra words to remove
  
  # words to remove
  blacklist <- c( 
    stopwords("english"),
    'รป',
    extra_blacklist)
  
  text %>% 
    VectorSource() %>%                  # makes a vector input formatted for corpus()
    Corpus() %>%                        # creates list of documents called a corpus
    tm_map(PlainTextDocument) %>%       # converts to plain text
    tm_map(tolower) %>%                 # makes all lowercase
    tm_map(removePunctuation) %>%       # remove punctuation
    tm_map(removeWords, blacklist) %>%  # remove blacklisted words
    tm_map(stemDocument) %>%            # combine stem words, ex: jumping/jumped
    DocumentTermMatrix() %>%            # converts to binary matrix
    removeSparseTerms(prob) %>%         # keeps top prob/99.5% words
    as.matrix() %>%
    as.tibble() %>% 
    suppressWarnings() %>% 
    return()
}
```

I will use the same function on text, keyword, and location to make it simple. This adds about 100 more variables than just using text.

```{r warning=FALSE}
n <- nrow(tweets)
test <- read_csv('./test.csv', show_col_types = FALSE)

all_tweets <- bind_rows(tweets, test)

x_text <- preprocess_tweets(all_tweets$text)
x_location <- preprocess_tweets(all_tweets$location)
x_keyword <- preprocess_tweets(all_tweets$keyword, 0.9999)

y_target <- tweets$target

# only for use in kaggle predictions
test_x_text <- x_text[(n + 1):nrow(all_tweets),]
test_x_location <- x_location[(n + 1):nrow(all_tweets),]
test_x_keyword <- x_keyword[(n + 1):nrow(all_tweets),]

x_text <- x_text[1:n,]
x_location <- x_location[1:n,]
x_keyword <- x_keyword[1:n,]
```


```{r}
tweets$text[1:4]
x_text[1:4, 1:10]
```

Splitting into validation set if training is too slow to use cross validation

```{r}
train_group <- sample(c(TRUE, FALSE), length(y_target), replace = TRUE, prob = c(0.75, 0.25))

train_x_text <- x_text[train_group,]
train_x_location <- x_location[train_group,]
train_x_keyword <- x_keyword[train_group,]

valid_x_text <- x_text[!train_group,]
valid_x_location <- x_location[!train_group,]
valid_x_keyword <- x_keyword[!train_group,]

train_y <- y_target[train_group]
valid_y <- y_target[!train_group]
```

I want to stack 3 random forest models for each of the three variables so there is better control over the weights of variables that would not be present in a single large random forest model.

```{r warning=FALSE}
library(randomForest)
rf.model.text <- randomForest(x = train_x_text, y = as.factor(train_y))
rf.model.location <- randomForest(x = train_x_location, y = as.factor(train_y))
rf.model.keyword <- randomForest(x = train_x_keyword, y = as.factor(train_y))

pred_y_text <- predict(rf.model.text, newdata = valid_x_text) %>% as.character() %>% as.numeric()
pred_y_location <- predict(rf.model.location, newdata = valid_x_location) %>% as.character() %>% as.numeric()
pred_y_keyword <- predict(rf.model.keyword, newdata = valid_x_keyword) %>% as.character() %>% as.numeric()

stacked.rf.lm <- lm(valid_y ~ pred_y_text + pred_y_keyword + pred_y_location, family = 'binomial')

pred_y = predict(
  stacked.rf.lm,
  newdata = data.frame(
    pred_y_text = as.numeric(as.character(predict(
      rf.model.text,
      newdata = valid_x_text
    ))),
    pred_y_location = as.numeric(as.character(predict(
      rf.model.location,
      newdata = valid_x_location
    ))),
    pred_y_keyword = as.numeric(as.character(predict(
      rf.model.keyword,
      newdata = valid_x_keyword
    )))
  )
)

accuracy_rf <- mean(round(pred_y) == valid_y)
accuracy_rf
```

```{r}
stacked.rf.lm
```

This model is not as accurate as I would like. I am not convinced that stacking the models gives any benefit. I would like to use a BERT model if I find an easy implementation to use in R.

In the future, I plan to use the whole dataset with CV, or tune the models better using a grid searching method. I also want to use the linked tweets as more predictors.

```{r warning=FALSE}
tibble(id = test$id,
       target = round(predict(
        stacked.rf.lm,
        newdata = data.frame(
          pred_y_text = as.numeric(as.character(predict(
            rf.model.text,
            newdata = test_x_text
          ))),
          pred_y_location = as.numeric(as.character(predict(
            rf.model.location,
            newdata = test_x_location
          ))),
          pred_y_keyword = as.numeric(as.character(predict(
            rf.model.keyword,
            newdata = test_x_keyword
          )))
        )
       ))) %>% 
  write_csv('./my_submission.csv')
```

